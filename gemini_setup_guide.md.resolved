# Guia de Implementa√ß√£o: Google Gemini AI

Este documento detalha como a integra√ß√£o com a IA do Google Gemini foi feita neste projeto ("Sync-finance-flow"), para que voc√™ possa replic√°-la em outros projetos.

## 1. Vis√£o Geral
A implementa√ß√£o atual utiliza a **API REST direta** do Google Generative AI (v1beta), o que permite maior controle e flexibilidade, embora a biblioteca oficial `@google/generative-ai` esteja listada nas depend√™ncias.

A solu√ß√£o √© composta por:
1.  **Servi√ßo Gemini ([gemini.js](file:///c:/Projetos/Sync-finance-flow/ai-server/gemini.js))**: Respons√°vel por montar o prompt, chamar a API e tratar a resposta.
2.  **Processador de Mensagens ([messageProcessor.js](file:///c:/Projetos/Sync-finance-flow/ai-server/messageProcessor.js))**: Orquestra o fluxo (recebe mensagem -> busca contexto -> chama AI -> salva no banco).

## 2. Pr√©-requisitos

### Vari√°veis de Ambiente
Voc√™ precisar√° de uma chave de API do Google AI Studio.
No arquivo [.env](file:///c:/Projetos/Sync-finance-flow/ai-server/.env):

```env
GEMINI_API_KEY=sua_chave_aqui_xyz
# ou
VITE_GEMINI_API_KEY=sua_chave_aqui_xyz
```

## 3. Estrutura de Arquivos Recomendada

```
/seuprujeto
  /ai-server (ou /services)
    gemini.js          # L√≥gica de conex√£o com a IA
    messageProcessor.js # L√≥gica de neg√≥cio e fluxo
```

## 4. Implementa√ß√£o Passo a Passo

### Passo 1: Servi√ßo Gemini ([gemini.js](file:///c:/Projetos/Sync-finance-flow/ai-server/gemini.js))

Este √© o cora√ß√£o da integra√ß√£o. Ele define o "System Prompt" (personalidade e regras) e gerencia a conex√£o.

**Pontos Chave:**
*   **System Prompt:** Define claramente que a IA deve responder **apenas JSON**.
*   **Fallback de Modelos:** Tenta usar o modelo mais novo (`gemini-2.5-flash`), se falhar, tenta vers√µes anteriores (`1.5-flash`, [pro](file:///c:/Projetos/Sync-finance-flow/ai-server/gemini.js#34-98)).
*   **Limpeza de Resposta:** Remove blocos de c√≥digo markdown (```json ... ```) antes de fazer o parse.

```javascript
import dotenv from 'dotenv';
dotenv.config();

// 1. Defini√ß√£o do Prompt do Sistema
const SYSTEM_PROMPT = `Voc√™ √© um assistente especializado.
Sua miss√£o √© analisar mensagens e convert√™-las em dados estruturados (JSON).

SEMPRE responda APENAS com um JSON v√°lido, sem markdown.

Estrutura do JSON esperado:
{
  "campo1": "valor",
  "campo2": "valor"
  // ... defina seus campos aqui
}
`;

export async function processMessageWithGemini(message, context = {}) {
  // 2. Obten√ß√£o da Chave
  const apiKey = process.env.VITE_GEMINI_API_KEY || process.env.GEMINI_API_KEY;
  if (!apiKey) return { error: 'ü§ñ GEMINI_API_KEY n√£o configurada.' };

  // 3. Modelos para tentar (em ordem de prefer√™ncia)
  const modelsToTry = ["gemini-2.5-flash", "gemini-1.5-flash", "gemini-pro"];
  let lastError = null;

  for (const modelName of modelsToTry) {
    try {
      console.log(`ü§ñ [FETCH] Tentando modelo: ${modelName}`);

      // 4. Montagem do Prompt com Contexto
      // Injetar dados din√¢micos ajuda a IA a ser mais precisa
      const prompt = `${SYSTEM_PROMPT}
      
CONTEXTO:
${JSON.stringify(context, null, 2)}

MENSAGEM DO USU√ÅRIO: "${message}"`;

      // 5. Chamada Direta via Fetch (REST API v1beta)
      const url = `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`;

      const response = await fetch(url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: prompt }] }]
        })
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(`API Error ${response.status}: ${data.error?.message || response.statusText}`);
      }

      // 6. Extra√ß√£o e Limpeza da Resposta
      const text = data.candidates?.[0]?.content?.parts?.[0]?.text;
      if (!text) throw new Error('Resposta vazia da IA');

      // Remove formata√ß√£o de c√≥digo se houver (```json ... ```)
      const jsonStr = text.replace(/```json/g, '').replace(/```/g, '').trim();
      
      return JSON.parse(jsonStr);

    } catch (error) {
      lastError = error;
      console.warn(`‚ö†Ô∏è [FETCH] Modelo ${modelName} falhou:`, error.message);
      // O loop continua para o pr√≥ximo modelo
    }
  }

  return { error: 'Desculpe, erro ao conectar com a intelig√™ncia.' };
}
```

### Passo 2: Uso no Fluxo ([messageProcessor.js](file:///c:/Projetos/Sync-finance-flow/ai-server/messageProcessor.js))

Como utilizar a fun√ß√£o criada acima dentro da l√≥gica do seu aplicativo.

```javascript
import { processMessageWithGemini } from './gemini.js';

export async function handleIncomingMessage(userMessage) {
  try {
    // 1. Prepare o contexto (dados do banco, status atual, etc)
    const context = {
      userStatus: 'active',
      lastAction: 'login'
    };

    // 2. Chame a IA
    const aiResult = await processMessageWithGemini(userMessage, context);

    // 3. Valide o resultado
    if (aiResult.error) {
      console.error(aiResult.error);
      return;
    }

    // 4. Use o JSON retornado
    console.log("Dados extra√≠dos:", aiResult);
    // Ex: salvar no banco, responder ao usu√°rio, etc.

  } catch (error) {
    console.error("Erro no processamento:", error);
  }
}
```

## 5. Dicas Importantes deste Projeto

1.  **JSON Mode**: Embora a API do Gemini tenha um modo JSON nativo, neste projeto optou-se por instruir via prompt ("SEMPRE responda APENAS com um JSON") e limpar a string manualmente (`text.replace...`). Isso provou ser muito robusto para modelos Flash.
2.  **Contexto √© Rei**: Observe em [gemini.js](file:///c:/Projetos/Sync-finance-flow/ai-server/gemini.js) (linha 45-48 do arquivo original) como listas de categorias, contas e cart√µes s√£o passadas no prompt. Isso permite que a IA "conhe√ßa" o banco de dados do usu√°rio e categorize corretamente sem alucinar nomes que n√£o existem.
3.  **Fallback de Modelos**: A estrat√©gia de tentar o `2.5-flash`, depois o `1.5-flash` e por fim o [pro](file:///c:/Projetos/Sync-finance-flow/ai-server/gemini.js#34-98) garante alta disponibilidade e tenta sempre usar o modelo mais r√°pido/inteligente primeiro.
